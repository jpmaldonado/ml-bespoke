{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Reinforcement Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning is a framework for tackling **sequential decision problems**: what to do next in order to maximize a reward (which might be delayed), on a changing universe (which might react to our actions).\n",
    "\n",
    "Concrete examples include:\n",
    "\n",
    "- Game playing: which actions are critical to win a game? This could be [Atari](http://karpathy.github.io/2016/05/31/rl/) or [Go](https://en.wikipedia.org/wiki/AlphaZero).\n",
    "- Learning in a \"small world\": what actions maximize pleasure / minimize pain?\n",
    "- [Treatment of chronic diseases](https://www.ncbi.nlm.nih.gov/pubmed/19731397): how to evolve the treatment for a disease that creates resistance?\n",
    "\n",
    "The unifying theme on the problems above can be abstracted as follows: \n",
    "- An **agent** receives a signal from the environment, selected by Nature. \n",
    "- The agent executes an **action**. \n",
    "- Given the agents' action, Nature assigns a reward and draws a new state, which is announced to the agent. \n",
    "- The situation repeats until a terminal criterion is reached.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the OpenAI Gym [(https://github.com/openai/gym)](https://github.com/openai/gym).  environment for this. \n",
    "\n",
    "It consists of a number of Atari environments that we can use for experimenting. If you haven't please install the library OpenAI gym (`pip install gym`).\n",
    "\n",
    "\n",
    "To test your installation, run the following script:\n",
    "\n",
    "```python\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "```    \n",
    "\n",
    "You should see a window that opens with a car and a pole, and will most likely close quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A walk on the Frozen Lake "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with a very simple environment, called `FrozenLake`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tqdm import tqdm\n",
    "import gym\n",
    "\n",
    "#create a single game instance\n",
    "env = gym.make(\"FrozenLake-v1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, **S** is the initial state, and your aim is to reach **G**, without falling into the holes, **H**. The squares marked with **F** are frozen, which means you can step on them.\n",
    " \n",
    " \n",
    "**Note:** The environment is non-deterministic, you can slip in the ice and end up in a different state.\n",
    "\n",
    "How to use the environment?\n",
    " \n",
    "- **reset()** returns the initial state / first observation.\n",
    "- **render()** returns the current environment state. \n",
    "- **step(a)** returns what happens after action a:\n",
    "     - *new observation*: the new state.\n",
    "     - *reward*: the reward corresponding to that action in that state.\n",
    "     - *is done*: binary flag, True if the game is over. \n",
    "     - *info*: Some auxiliary stuff, which we can ignore now.\n",
    "     \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial state:  (0, {'prob': 1})\n",
      " and it looks like: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nobleprog/Desktop/ml-bespoke/env/lib/python3.10/site-packages/gym/envs/toy_text/frozen_lake.py:271: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"FrozenLake-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"The initial state: \", env.reset())\n",
    "print(\" and it looks like: \")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now let's take an action: \n"
     ]
    }
   ],
   "source": [
    "print(\"Now let's take an action: \")\n",
    "env.reset()\n",
    "new_state, reward, done, prob, _ = env.step(1)\n",
    "env.render()\n",
    "\n",
    "\n",
    "idx_to_action = {\n",
    "    0:\"<\", #left\n",
    "    1:\"v\", #down\n",
    "    2:\">\", #right\n",
    "    3:\"^\" #up\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **policy** is a function from states to actions. It tells us what we should do on each state. In this case, an array of size 16 with entries 0,1,2 or 3 determines a **deterministic** policy, whereas an array of size 16x4 with entries between 0 and 1 and where each row sums 1 determines a **stochastic** policy.\n",
    " \n",
    "For simplicity, we will implement policies as dictionaries for `FrozenLake`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Initialize random_policy:\n",
    "def init_random_policy():\n",
    "    random_policy  = {}\n",
    "    for state in range(n_states):\n",
    "        random_policy[state] = np.random.choice(n_actions)\n",
    "    return random_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We need now to define a function to evaluate our policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, {'prob': 1})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, policy, max_episodes=100): \n",
    "    tot_reward = 0\n",
    "    for ep in range(max_episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        ep_reward = 0\n",
    "        \n",
    "        # Reward per episode\n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "            new_state, reward, done, _, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "            state = new_state\n",
    "            if done:\n",
    "                tot_reward += ep_reward\n",
    "    return tot_reward/(max_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for the best policy: Random search\n",
    "\n",
    "As a very first example, let's try to find our policy by pure random search: we will play for some time and keep track of the best actions we can do on each state.\n",
    "\n",
    "`FrozenLake` defines \"solving\" as getting average reward of 0.78 over 100 consecutive trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_policy = None\n",
    "best_score = -float('inf')\n",
    "\n",
    "# Random search\n",
    "for i in range(1,10000): #tip: you can use tqdm(range(1,10000)) for a progress bar\n",
    "    policy = init_random_policy()\n",
    "    score = evaluate(env,policy,100)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_policy = policy\n",
    "    if i%5000 == 0:\n",
    "        print(\"Best score:\", best_score)\n",
    "print(\"Best policy:\")\n",
    "#print(best_policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the policy in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env,policy, render=False):\n",
    "    s = env.reset()\n",
    "    d = False\n",
    "    while not d:\n",
    "        a = policy[s]\n",
    "        print(\"*\"*10)\n",
    "        print(\"State: \",s)\n",
    "        print(\"Action: \",idx_to_action[a])\n",
    "        s, r, d, _ = env.step(a)\n",
    "        if render:\n",
    "            env.render()\n",
    "        if d:\n",
    "            print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a small function to print a nicer policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(policy):\n",
    "    lake = \"SFFFFHFHFFFHHFFG\"\n",
    "    arrows = [idx_to_action[policy[i]] \n",
    "               if lake[i] in 'SF' \\\n",
    "              else '*' for i in range(n_states)]\n",
    "    for i in range(0,16,4):\n",
    "        print(''.join(arrows[i:i+4]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call then use the functions above to render the optimal policy. Note that the policy might not give the optimal action: recall that there is some noise involved (you can slip on the ice) which is responsible of a non-optimal action looking like optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_policy(best_policy)\n",
    "\n",
    "#play(env,best_policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a different policy\n",
    "\n",
    "Let's try some different implementation of a random policy, which will be more useful later on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta = 0.25*np.ones((n_states,n_actions))\n",
    "def random_parameter_policy(theta):\n",
    "    theta = theta/np.sum(theta, axis=1, keepdims=True) # ensure that the array is normalized\n",
    "    policy  = {}\n",
    "    probs = {}\n",
    "    for state in range(n_states):\n",
    "        probs[state] = np.array(theta[state,:])\n",
    "        policy[state] = np.random.choice(n_actions, p = probs[state])\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_policy = None\n",
    "best_score = -float('inf')\n",
    "alpha = 1e-2\n",
    "\n",
    "theta = 0.25*np.ones((n_states,n_actions))\n",
    "\n",
    "# Random search\n",
    "for i in range(1,10000): \n",
    "    policy = random_parameter_policy(theta)\n",
    "    score = evaluate(env,policy,100)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_policy = policy\n",
    "    theta = theta + alpha*np.random.rand(n_states,n_actions)\n",
    "    if i%5000 == 0:\n",
    "        print(\"Best score:\", best_score)\n",
    "#print(\"Best policy:\")\n",
    "#print(best_policy)\n",
    "#print(\"Best score:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the advantage of this? Perhaps not much right now, but this is the first step to use more sophisticated techniques over random search. Note that we do a \"gradient update\" of sorts when we change the parameter `theta` in the direction of increase of the best score. This hints that we could use other update rules, perhaps taking the output as a more sophisticated input of the game history.\n",
    "\n",
    "Another thing to notice is that we made effectively our policy **stochastic**: at every stage the agent has the possibility of choosing randomly his action. This has the effect of smoothing out the problem: we are now solving an optimization problem on a continuous, instead of a discrete space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn:\n",
    "\n",
    "- Beat the hill climbing / random search benchmark! Implement a different search method for the parameters.\n",
    "- Try the `CartPole` environment. In `CartPole`, the state is continuous (4 different parameters), so you need to do something on the lines of the parameterized random search example. Look at http://kvfrans.com/simple-algoritms-for-solving-cartpole/ for inspiration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
